# AI Agent Orchestrator

A multi-agent backend system that coordinates specialized LLM-powered agents to handle complex IT diagnostics and IT engineering workflows through a single HTTP API.

> **Template Repository**: This is a public template that can be used as a foundation for building AI agent orchestration systems. It includes security features, deployment configurations, and integration examples.

## Overview

The AI Agent Orchestrator is a FastAPI-based system that enables coordination of multiple specialized AI agents for IT operations. Each agent is designed to handle specific types of tasks (network diagnostics, system monitoring, log analysis, infrastructure management) and can work independently or collaboratively through the orchestrator.

### Use Cases

- **Chatbot Enhancement**: Integrate specialized agents into existing chatbots for IT diagnostics and troubleshooting
- **IT Operations**: Automate network diagnostics, system monitoring, and log analysis
- **Multi-Agent Workflows**: Coordinate multiple agents to handle complex, multi-step tasks
- **API Service**: Provide agent orchestration as a service to other applications

## Architecture

### Core Components

- **Orchestrator**: Routes tasks to appropriate agents and coordinates multi-agent workflows
- **Agent Registry**: Manages available agents and their capabilities
- **Workflow Executor**: Executes multi-step workflows involving multiple agents
- **LLM Manager**: Manages LLM provider selection and initialization
- **Message Bus**: Enables agent-to-agent communication

### Agents

- **Network Diagnostics Agent**: Handles network connectivity, latency, routing, and DNS issues
- **System Monitoring Agent**: Monitors CPU, memory, disk usage, and processes
- **Log Analysis Agent**: Analyzes logs, detects errors, and provides troubleshooting insights
- **Infrastructure Agent**: Handles provisioning, configuration management, and deployment

### LLM Providers

- **AWS Bedrock** (Primary): Claude 3 Haiku - Low-cost, serverless option
- **OpenAI**: GPT-3.5-turbo - Alternative provider
- **Ollama**: Local/open-source models - Free, self-hosted option

## Setup

### Prerequisites

- Python 3.9 or higher
- pip or poetry for dependency management
- AWS credentials (if using Bedrock)
- OpenAI API key (if using OpenAI)
- Ollama installed locally (if using Ollama)

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd ai-agent-orchestrator
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Configure environment variables:
```bash
cp .env.example .env
# Edit .env with your configuration
```

### Environment Variables

Key environment variables to configure:

- `LLM_PROVIDER`: Provider to use (bedrock, openai, ollama)
- `AWS_REGION`: AWS region for Bedrock
- `AWS_ACCESS_KEY_ID`: AWS access key
- `AWS_SECRET_ACCESS_KEY`: AWS secret key
- `OPENAI_API_KEY`: OpenAI API key (if using OpenAI)
- `CORS_ORIGINS`: Comma-separated list of allowed origins

## Running the Application

### Development Mode

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Production Mode

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

The API will be available at:
- API: http://localhost:8000
- Documentation: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

## API Endpoints

### Health Check

```http
GET /api/v1/health
```

Returns the health status of the application.

### Orchestrate Task

```http
POST /api/v1/orchestrate
Content-Type: application/json

{
  "task": "Diagnose network connectivity issues",
  "context": {
    "hostname": "example.com",
    "port": 443
  }
}
```

Submits a task to the orchestrator for execution by appropriate agents.

### List Agents

```http
GET /api/v1/agents
```

Returns a list of all available agents and their capabilities.

### Get Agent Details

```http
GET /api/v1/agents/{agent_id}
```

Returns detailed information about a specific agent.

### Execute Workflow

```http
POST /api/v1/workflows
Content-Type: application/json

{
  "workflow_id": "network_diagnostics_workflow",
  "input_data": {
    "target": "example.com"
  }
}
```

Executes a predefined multi-step workflow.

## ðŸš€ Quick Start for Chatbot Integration

If you're integrating this into an existing chatbot (like donsylvester.dev), see:

- **[CHATBOT_SETUP.md](CHATBOT_SETUP.md)** - Quick setup checklist
- **[INTEGRATION_GUIDE.md](INTEGRATION_GUIDE.md)** - Detailed integration guide with code examples
- **[examples/](examples/)** - Ready-to-use code examples for backend proxy and frontend integration

## ðŸ“š Documentation

### Getting Started
- **[CHATBOT_SETUP.md](CHATBOT_SETUP.md)** - Quick setup checklist for chatbot integration
- **[INTEGRATION_GUIDE.md](INTEGRATION_GUIDE.md)** - Detailed integration guide with code examples
- **[SETUP_SUMMARY.md](SETUP_SUMMARY.md)** - Complete setup requirements list
- **[ADDING_AGENTS.md](ADDING_AGENTS.md)** - Step-by-step guide to create and register new agents

### Deployment
- **[DEPLOYMENT.md](DEPLOYMENT.md)** - Production deployment guide
- **[DEPLOYMENT_INTEGRATION.md](DEPLOYMENT_INTEGRATION.md)** - Integration with existing AWS infrastructure
- **[EXISTING_INFRASTRUCTURE_ANALYSIS.md](EXISTING_INFRASTRUCTURE_ANALYSIS.md)** - Analysis of current AWS setup

### Production Readiness
- **[PRODUCTION_ROADMAP.md](PRODUCTION_ROADMAP.md)** - â­ **Start Here** - Quick reference roadmap
- **[ENTERPRISE_READINESS.md](ENTERPRISE_READINESS.md)** - Requirements for small business to enterprise
- **[SCALABILITY_ARCHITECTURE.md](SCALABILITY_ARCHITECTURE.md)** - Scalability patterns and architecture
- **[PRODUCTION_READINESS.md](PRODUCTION_READINESS.md)** - Detailed production readiness review
- **[PRODUCTION_GAPS.md](PRODUCTION_GAPS.md)** - Quick summary of missing features

### Security & Infrastructure
- **[SECURITY.md](SECURITY.md)** - Security features and best practices
- **[AWS_INFRASTRUCTURE_REVIEW.md](AWS_INFRASTRUCTURE_REVIEW.md)** - AWS infrastructure recommendations
- **[AWS_INFRASTRUCTURE_ANALYSIS.md](AWS_INFRASTRUCTURE_ANALYSIS.md)** - Analysis of your AWS setup

### Architecture
- **[ARCHITECTURE.md](ARCHITECTURE.md)** - System architecture and design

## âš ï¸ Current Status

**This is a template/scaffolding repository.** Core business logic needs to be implemented before production use. See [PRODUCTION_GAPS.md](PRODUCTION_GAPS.md) for details.

**What's Ready:**
- âœ… Security features (API keys, rate limiting, CORS)
- âœ… Configuration management
- âœ… Docker deployment
- âœ… Code structure and architecture

**What Needs Implementation:**
- âš ï¸ Core business logic (orchestrator, agents, LLM providers)
- âš ï¸ Logging and error handling
- âš ï¸ Database/persistence layer
- âš ï¸ Monitoring and observability
- âš ï¸ Testing infrastructure

## Usage Examples

### Example 1: Network Diagnostics

```python
import requests

response = requests.post(
    "http://localhost:8000/api/v1/orchestrate",
    json={
        "task": "Check connectivity to example.com on port 443",
        "context": {
            "hostname": "example.com",
            "port": 443
        }
    }
)

result = response.json()
print(result)
```

### Example 2: System Monitoring

```python
response = requests.post(
    "http://localhost:8000/api/v1/orchestrate",
    json={
        "task": "Monitor CPU and memory usage",
        "context": {
            "duration": 60,
            "interval": 5
        }
    }
)
```

### Example 3: Log Analysis

```python
response = requests.post(
    "http://localhost:8000/api/v1/orchestrate",
    json={
        "task": "Analyze error logs and identify root cause",
        "context": {
            "log_file": "/var/log/app/error.log",
            "time_range": "last_24_hours"
        }
    }
)
```

## Project Structure

```
ai-agent-orchestrator/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI application entry point
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ v1/
â”‚   â”‚       â””â”€â”€ routes/         # API route handlers
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py           # Configuration management
â”‚   â”‚   â”œâ”€â”€ orchestrator.py     # Orchestration engine
â”‚   â”‚   â”œâ”€â”€ agent_registry.py   # Agent registry
â”‚   â”‚   â”œâ”€â”€ workflow_executor.py # Workflow executor
â”‚   â”‚   â””â”€â”€ messaging.py        # Message bus
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ base.py             # Base agent class
â”‚   â”‚   â”œâ”€â”€ network_diagnostics.py
â”‚   â”‚   â”œâ”€â”€ system_monitoring.py
â”‚   â”‚   â”œâ”€â”€ log_analysis.py
â”‚   â”‚   â””â”€â”€ infrastructure.py
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ base.py             # LLM provider interface
â”‚   â”‚   â”œâ”€â”€ bedrock.py          # AWS Bedrock provider
â”‚   â”‚   â”œâ”€â”€ openai.py           # OpenAI provider
â”‚   â”‚   â”œâ”€â”€ ollama.py           # Ollama provider
â”‚   â”‚   â””â”€â”€ manager.py          # LLM manager
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ agent.py            # Agent data models
â”‚   â”‚   â”œâ”€â”€ workflow.py         # Workflow data models
â”‚   â”‚   â””â”€â”€ request.py          # API request/response models
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ examples/           # Example workflow definitions
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ agents.yaml             # Agent configurations
â”‚   â””â”€â”€ llm.yaml                # LLM provider configurations
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## Development

### Implementing Agents

All agents inherit from `BaseAgent` and must implement the `execute` method:

```python
from app.agents.base import BaseAgent
from app.models.agent import AgentResult

class MyAgent(BaseAgent):
    async def execute(self, task: str, context: Optional[Dict] = None) -> AgentResult:
        # Implement agent logic here
        result = await self._generate_response(prompt)
        return self._format_result(success=True, output=result)
```

### Implementing LLM Providers

All LLM providers inherit from `LLMProvider` and must implement:

- `generate()`: Generate text response
- `stream()`: Stream text response
- `generate_with_metadata()`: Generate response with usage metadata

### Adding Workflows

Workflows are defined as YAML or JSON files in the `app/workflows/` directory. Each workflow consists of multiple steps that can be executed by different agents.

## Configuration

### Agent Configuration

Edit `config/agents.yaml` to configure agent capabilities and settings.

### LLM Configuration

Edit `config/llm.yaml` to configure LLM provider settings and defaults.

## Testing

```bash
# Run tests (when implemented)
pytest
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## License

[Specify your license here]

## Support

For issues and questions, please open an issue on the repository.

