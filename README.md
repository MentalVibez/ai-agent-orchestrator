# AI Agent Orchestrator

A multi-agent backend system that coordinates specialized LLM-powered agents to handle complex IT diagnostics and IT engineering workflows through a single HTTP API. **Now with MCP (Model Context Protocol)**: register MCP servers, define agent profiles, and run goal-based workflows that compose tools from multiple MCP servers (or fall back to legacy agents when no MCP tools are configured).

> **Production-Ready System**: This is a fully functional, production-ready AI agent orchestration system. **All core functionality is implemented** including 3 working agents (Network Diagnostics, System Monitoring, Code Review), tool system, dynamic prompts, database persistence, workflows, cost tracking, MCP client layer, and comprehensive testing. See [ADDING_AGENTS.md](ADDING_AGENTS.md) to extend with additional agents.

---

## Quick start (step-by-step)

Follow these steps in order. If something fails, check the error message and the [Setup](#setup) section below.

| Step | What to do | Command (Windows) | Command (Linux / macOS) |
|------|------------|-------------------|--------------------------|
| 1 | Open a terminal in the project folder | `cd path\to\ai-agent-orchestrator` | `cd /path/to/ai-agent-orchestrator` |
| 2 | Create a virtual environment | `python -m venv venv` | `python3 -m venv venv` |
| 3 | Activate the virtual environment | `venv\Scripts\activate` | `source venv/bin/activate` |
| 4 | Install dependencies | `python -m pip install -r requirements.txt` | `pip install -r requirements.txt` |
| 5 | Copy the env template to `.env` | `copy env.template .env` | `cp env.template .env` |
| 6 | Edit `.env` and set at least `API_KEY` and (for Bedrock) `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION` | Use Notepad or any editor | Use nano, vim, or any editor |
| 7 | Start the API server | `python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000` | `uvicorn app.main:app --reload --host 0.0.0.0 --port 8000` |
| 8 | Check it works | Open in browser: **http://localhost:8000/docs** and **http://localhost:8000/api/v1/health** | Same |

- **No `.env.example`?** This project uses **`env.template`**. Copy it to **`.env`** (see step 5).
- **Port 8000 in use?** Change `--port 8000` to another port (e.g. `--port 8080`) in step 7.
- **Tests:** From the project root with the venv activated, run: `python -m pytest tests/ -v --tb=short`. Optional: `python -m ruff check app/ tests/` and `python -m pip_audit` for lint and security.
- **Before pushing to GitHub:** Run the test suite and fix any failures; run `pip-audit` (or your security check) and fix critical issues. Then commit and push.

## Overview

The AI Agent Orchestrator is a FastAPI-based system that enables coordination of multiple specialized AI agents for IT operations. Each agent is designed to handle specific types of tasks (network diagnostics, system monitoring, log analysis, infrastructure management) and can work independently or collaboratively through the orchestrator.

### Use Cases

- **Chatbot Enhancement**: Integrate specialized agents into existing chatbots for IT diagnostics and troubleshooting
- **IT Operations**: Automate network diagnostics, system monitoring, and log analysis
- **Multi-Agent Workflows**: Coordinate multiple agents to handle complex, multi-step tasks
- **API Service**: Provide agent orchestration as a service to other applications

## Architecture

### Core Components

- **Orchestrator**: Routes tasks to appropriate agents and coordinates multi-agent workflows
- **Agent Registry**: Manages available agents and their capabilities
- **Workflow Executor**: Executes multi-step workflows involving multiple agents
- **LLM Manager**: Manages LLM provider selection and initialization
- **Message Bus**: Enables agent-to-agent communication

### Agents

- **Network Diagnostics Agent**: Handles network connectivity, latency, routing, and DNS issues
- **System Monitoring Agent**: Monitors CPU, memory, disk usage, and processes
- **Code Review Agent**: Performs security analysis, code quality review, and vulnerability detection (NEW)
- **Log Analysis Agent**: Analyzes logs, detects errors, and provides troubleshooting insights
- **Infrastructure Agent**: Handles provisioning, configuration management, and deployment

### LLM Providers

- **AWS Bedrock** (Primary): Claude 3 Haiku - Low-cost, serverless option
- **OpenAI**: GPT-3.5-turbo - Alternative provider
- **Ollama**: Local/open-source models - Free, self-hosted option

### MCP-Centric Runs (New)

- **MCP Client Manager**: Connects to multiple MCP servers (stdio) at startup, discovers tools, and routes tool calls. Configure servers in `config/mcp_servers.yaml`.
- **Agent Profiles**: Define profiles (role prompt, allowed MCP servers) in `config/agent_profiles.yaml`. Each run uses one profile.
- **Planner Loop**: For a run, the planner LLM repeatedly chooses the next action (call an MCP tool or FINISH with an answer). If no MCP tools are enabled for the profile, the run uses the **legacy orchestrator** (existing agents) and returns that result.
- **Runs API**: `POST /api/v1/run` with `{ "goal": "...", "agent_profile_id": "default" }` starts a run and returns `run_id`. Poll `GET /api/v1/runs/{run_id}` for status, steps, tool calls, and final answer.

## Setup

### Prerequisites

- Python 3.9 or higher
- pip or poetry for dependency management
- AWS credentials (if using Bedrock)
- OpenAI API key (if using OpenAI)
- Ollama installed locally (if using Ollama)

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd ai-agent-orchestrator
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Configure environment variables (use the template file; do not commit `.env`):
   - **Windows:** `copy env.template .env`
   - **Linux / macOS:** `cp env.template .env`
   Then edit `.env` and set at least `API_KEY` and your LLM provider keys (e.g. AWS or OpenAI).

### Environment Variables

Key environment variables to configure:

- **LLM:** `LLM_PROVIDER` (bedrock, openai, ollama), `AWS_REGION`, `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`; or `OPENAI_API_KEY` for OpenAI
- **Security:** `API_KEY`, `REQUIRE_API_KEY`, `AGENT_WORKSPACE_ROOT` (restrict file tools), `PROMPT_INJECTION_FILTER_ENABLED`
- **Planner:** `PLANNER_LLM_TIMEOUT_SECONDS` (default 120; 0 = no timeout)
- **App:** `CORS_ORIGINS` (comma-separated), `LOG_LEVEL`, `DEBUG`

## Running the Application

### Development Mode

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Production Mode

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

The API will be available at:
- **API:** http://localhost:8000
- **Docs (Swagger):** http://localhost:8000/docs
- **ReDoc:** http://localhost:8000/redoc
- **Console (goal-based runs):** http://localhost:8000/console

## API Endpoints

### Health Check

```http
GET /api/v1/health
```

Returns the health status of the application.

### Orchestrate Task

```http
POST /api/v1/orchestrate
Content-Type: application/json

{
  "task": "Diagnose network connectivity issues",
  "context": {
    "hostname": "example.com",
    "port": 443
  }
}
```

Submits a task to the orchestrator for execution by appropriate agents.

### List Agents

```http
GET /api/v1/agents
```

Returns a list of all available agents and their capabilities.

### Get Agent Details

```http
GET /api/v1/agents/{agent_id}
```

Returns detailed information about a specific agent.

### Execute Workflow

```http
POST /api/v1/workflows
Content-Type: application/json

{
  "workflow_id": "network_diagnostics_workflow",
  "input_data": {
    "target": "example.com"
  }
}
```

Executes a predefined multi-step workflow.

### MCP-Centric Run (goal-based)

```http
POST /api/v1/run
Content-Type: application/json

{
  "goal": "Check connectivity to example.com on port 443",
  "agent_profile_id": "default",
  "context": {}
}
```

Returns `{ "run_id": "...", "status": "pending", ... }`. Then poll:

```http
GET /api/v1/runs/{run_id}
```

Returns run status, steps, tool calls, and `answer` when completed.

- **GET /api/v1/agent-profiles** ‚Äì List enabled agent profiles (id, name, description).
- **GET /api/v1/mcp/servers** ‚Äì List connected MCP servers and their exposed tools (governance/transparency).

**Playwright (browser automation):** Use `"agent_profile_id": "browser"` for goals that need browser automation. **Fetch:** HTTP fetch is enabled; use with **Deep Research** (see below). **Deep Research profile:** Use `"agent_profile_id": "deep_research"` to combine Fetch and Playwright (fetch URLs, automate the browser, synthesize answers). Requires Node.js 18+ for MCP servers.

**Personal Multi-Agent Console:** Open `/console` in the browser (e.g. http://localhost:8000/console) to enter a goal, pick an agent profile, start a run, and watch status, steps, and the final answer. Set the API key in the form if the server requires it.

## üöÄ Quick Start for Chatbot Integration

If you're integrating this into an existing chatbot (like donsylvester.dev), see:

- **[CHATBOT_SETUP.md](CHATBOT_SETUP.md)** - Quick setup checklist
- **[INTEGRATION_GUIDE.md](INTEGRATION_GUIDE.md)** - Detailed integration guide with code examples
- **[examples/](examples/)** - Ready-to-use code examples for backend proxy and frontend integration

## üìö Documentation

### Getting Started
- **[CONTRIBUTING.md](CONTRIBUTING.md)** - How to contribute; run tests; add MCP servers and agent profiles
- **[ADDING_AGENTS.md](ADDING_AGENTS.md)** - Step-by-step guide to create and register new agents
- **[CHATBOT_SETUP.md](CHATBOT_SETUP.md)** - Quick setup checklist for chatbot integration
- **[INTEGRATION_GUIDE.md](INTEGRATION_GUIDE.md)** - Detailed integration guide with code examples
- **[SETUP_SUMMARY.md](SETUP_SUMMARY.md)** - Complete setup requirements list
- **[STATUS_UPDATE.md](STATUS_UPDATE.md)** - Current implementation status and assessment
- **[QUALITY_SUGGESTIONS.md](QUALITY_SUGGESTIONS.md)** - Backlog of quality and CI improvements

### Deployment
- **[DEPLOYMENT.md](DEPLOYMENT.md)** - Production deployment guide
- **[DEPLOYMENT_INTEGRATION.md](DEPLOYMENT_INTEGRATION.md)** - Integration with existing AWS infrastructure
- **[EXISTING_INFRASTRUCTURE_ANALYSIS.md](EXISTING_INFRASTRUCTURE_ANALYSIS.md)** - Analysis of current AWS setup

### Production Readiness
- **[PRODUCTION_ROADMAP.md](PRODUCTION_ROADMAP.md)** - ‚≠ê **Start Here** - Quick reference roadmap
- **[ENTERPRISE_READINESS.md](ENTERPRISE_READINESS.md)** - Requirements for small business to enterprise
- **[SCALABILITY_ARCHITECTURE.md](SCALABILITY_ARCHITECTURE.md)** - Scalability patterns and architecture
- **[PRODUCTION_READINESS.md](PRODUCTION_READINESS.md)** - Detailed production readiness review
- **[PRODUCTION_GAPS.md](PRODUCTION_GAPS.md)** - Quick summary of missing features

### Security & Infrastructure
- **[SECURITY.md](SECURITY.md)** - Security features and best practices
- **[AWS_INFRASTRUCTURE_REVIEW.md](AWS_INFRASTRUCTURE_REVIEW.md)** - AWS infrastructure recommendations
- **[AWS_INFRASTRUCTURE_ANALYSIS.md](AWS_INFRASTRUCTURE_ANALYSIS.md)** - Analysis of your AWS setup

### Architecture & New Features
- **[ARCHITECTURE.md](ARCHITECTURE.md)** - System architecture and design
- **[CODE_REVIEW_AGENT_ANALYSIS.md](CODE_REVIEW_AGENT_ANALYSIS.md)** - Analysis of code review agent patterns
- **[TOOLS_AND_CODE_REVIEW_IMPLEMENTATION.md](TOOLS_AND_CODE_REVIEW_IMPLEMENTATION.md)** - Tools and code review implementation details
- **[MONITORING.md](MONITORING.md)** - Monitoring and observability guide
- **[DEPLOYMENT_K8S.md](DEPLOYMENT_K8S.md)** - Kubernetes deployment guide

## ‚úÖ Current Status

**üü¢ Production-Ready with Advanced Features**

The system is **fully implemented and production-ready** with comprehensive features including tool system, code review capabilities, dynamic prompts, database persistence, workflows, cost tracking, testing, and monitoring. All core functionality is working and tested.

**‚úÖ What's Implemented and Working:**
- ‚úÖ **Core**: Agent Registry, Orchestrator routing, LLM Provider (Bedrock), tool system (file read, code search, dir list) with workspace sandboxing
- ‚úÖ **Agents**: Network Diagnostics, System Monitoring, Code Review (3 fully functional)
- ‚úÖ **MCP**: Client manager (stdio), configurable servers (`config/mcp_servers.yaml`), agent profiles (`config/agent_profiles.yaml`), goal-based runs API (POST /run, GET /runs/:id), Personal Console at `/console`
- ‚úÖ **Planner**: LLM loop with timeout and run cancellation; optional prompt-injection filter and structural hardening
- ‚úÖ **API**: Orchestrate, agents, workflows, cost metrics, runs (start, list, get, cancel), agent-profiles, mcp/servers, health
- ‚úÖ **Security**: API key auth, rate limiting, CORS, security headers, agent sandboxing, workspace root restriction, [SECURITY.md](SECURITY.md)
- ‚úÖ **Quality**: Ruff lint, tests (unit + integration), optional mypy and pip-audit; see [CONTRIBUTING.md](CONTRIBUTING.md) and [QUALITY_SUGGESTIONS.md](QUALITY_SUGGESTIONS.md)
- ‚úÖ **Database**: SQLite for execution history, agent state, runs
- ‚úÖ **Deployment**: Docker, CloudFormation, Kubernetes manifests, AWS guides

**‚ö†Ô∏è Optional/Advanced Features:**
- ‚ö†Ô∏è Additional agents (Log Analysis, Infrastructure - available as templates)
- ‚ö†Ô∏è Additional LLM providers (OpenAI, Ollama - Bedrock is fully working)
- ‚ö†Ô∏è Advanced monitoring dashboards (Prometheus metrics available)
- ‚ö†Ô∏è PostgreSQL support (SQLite works for MVP)

**üìä Progress:**
- **Core Functionality**: ‚úÖ 100% Complete
- **Production Features**: ‚úÖ 100% Complete  
- **API Endpoints**: ‚úÖ 100% Complete (all endpoints working)
- **Agents**: ‚úÖ 3/5 implemented (Network Diagnostics, System Monitoring, Code Review)
- **Tool System**: ‚úÖ 100% Complete (4 core tools implemented)
- **Dynamic Prompts**: ‚úÖ 100% Complete
- **Workflows**: ‚úÖ 100% Complete
- **Database**: ‚úÖ 100% Complete
- **Testing**: ‚úÖ 100% Complete
- **LLM Providers**: ‚úÖ 1/3 implemented (Bedrock - others optional)

**üéØ Ready For:**
- ‚úÖ Small business production use
- ‚úÖ Chatbot integration
- ‚úÖ IT diagnostics and troubleshooting
- ‚úÖ Code review and security analysis
- ‚úÖ Multi-step workflow automation
- ‚úÖ Extending with custom agents

**üìù To Extend:**
- See [ADDING_AGENTS.md](ADDING_AGENTS.md) to add more agents
- See [PRODUCTION_REMAINING.md](PRODUCTION_REMAINING.md) for optional enhancements

## Usage Examples

### Example 1: Code Review (Security Analysis)

```python
import requests

# Review code for security vulnerabilities
response = requests.post(
    "http://localhost:8000/api/v1/orchestrate",
    headers={"X-API-Key": "your-api-key"},
    json={
        "task": "Review code for security vulnerabilities",
        "context": {
            "directory": "app",
            "focus_areas": ["security", "quality"]
        }
    }
)

result = response.json()
print(f"Security Issues Found: {result['results'][0]['output']['issues_found']}")
```

### Example 2: MCP goal-based run (recommended)

```python
import requests
import time

# Start a run
r = requests.post(
    "http://localhost:8000/api/v1/run",
    headers={"X-API-Key": "your-api-key"},
    json={"goal": "Check connectivity to example.com on port 443", "agent_profile_id": "default"},
)
run = r.json()
run_id = run["run_id"]

# Poll until completed
while run["status"] not in ("completed", "failed", "cancelled"):
    time.sleep(1)
    r = requests.get(f"http://localhost:8000/api/v1/runs/{run_id}", headers={"X-API-Key": "your-api-key"})
    run = r.json()
print(run.get("answer", run))
```

### Example 3: Network Diagnostics (legacy orchestrate)

```python
import requests

response = requests.post(
    "http://localhost:8000/api/v1/orchestrate",
    json={
        "task": "Check connectivity to example.com on port 443",
        "context": {
            "hostname": "example.com",
            "port": 443
        }
    }
)

result = response.json()
print(result)
```

### Example 4: System Monitoring

```python
response = requests.post(
    "http://localhost:8000/api/v1/orchestrate",
    json={
        "task": "Monitor CPU and memory usage",
        "context": {
            "duration": 60,
            "interval": 5
        }
    }
)
```

### Example 5: Log Analysis

```python
response = requests.post(
    "http://localhost:8000/api/v1/orchestrate",
    json={
        "task": "Analyze error logs and identify root cause",
        "context": {
            "log_file": "/var/log/app/error.log",
            "time_range": "last_24_hours"
        }
    }
)
```

## Project Structure

```
ai-agent-orchestrator/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # FastAPI application entry point
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ v1/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ routes/         # API route handlers
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py           # Configuration management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py     # Orchestration engine
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent_registry.py   # Agent registry
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run_store.py        # Run persistence (MCP runs)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompt_injection.py # Optional input filter
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ workflow_executor.py # Workflow executor
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ messaging.py        # Message bus
‚îÇ   ‚îú‚îÄ‚îÄ planner/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ loop.py             # MCP planner loop (goal ‚Üí tool calls or finish)
‚îÇ   ‚îú‚îÄ‚îÄ mcp/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config_loader.py   # Load MCP servers & agent profiles
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ client_manager.py   # MCP client (stdio, tool discovery)
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py             # Base agent class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ network_diagnostics.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ system_monitoring.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ log_analysis.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ infrastructure.py
‚îÇ   ‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py             # LLM provider interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bedrock.py          # AWS Bedrock provider
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai.py           # OpenAI provider
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama.py           # Ollama provider
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ manager.py          # LLM manager
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent.py            # Agent data models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run.py              # Run request/response models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ workflow.py         # Workflow data models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ request.py          # API request/response models
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ examples/           # Example workflow definitions
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ agents.yaml             # Agent configurations
‚îÇ   ‚îú‚îÄ‚îÄ llm.yaml                # LLM provider configurations
‚îÇ   ‚îú‚îÄ‚îÄ mcp_servers.yaml        # MCP server definitions (stdio)
‚îÇ   ‚îî‚îÄ‚îÄ agent_profiles.yaml     # Agent profiles (role prompt, allowed MCP servers)
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ env.template          # Copy to .env and configure (do not commit .env)
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

## Development

- **Contributing:** See [CONTRIBUTING.md](CONTRIBUTING.md) for setup, running tests (`pytest tests/`), adding MCP servers and agent profiles, and optional lint/type check (Ruff, mypy).
- **Quality backlog:** See [QUALITY_SUGGESTIONS.md](QUALITY_SUGGESTIONS.md) for improvement ideas.

### Implementing Agents

All agents inherit from `BaseAgent` and must implement the `execute` method:

```python
from app.agents.base import BaseAgent
from app.models.agent import AgentResult

class MyAgent(BaseAgent):
    async def execute(self, task: str, context: Optional[Dict] = None) -> AgentResult:
        # Implement agent logic here
        result = await self._generate_response(prompt)
        return self._format_result(success=True, output=result)
```

### Implementing LLM Providers

All LLM providers inherit from `LLMProvider` and must implement:

- `generate()`: Generate text response
- `stream()`: Stream text response
- `generate_with_metadata()`: Generate response with usage metadata

### Adding Workflows

Workflows are defined as YAML or JSON files in the `app/workflows/` directory. Each workflow consists of multiple steps that can be executed by different agents.

## Configuration

### Agent Configuration

Edit `config/agents.yaml` to configure agent capabilities and settings.

### LLM Configuration

Edit `config/llm.yaml` to configure LLM provider settings and defaults.

## Testing

```bash
# Run all tests (from project root with venv activated)
python -m pytest tests/ -v --tb=short

# Optional: lint and security audit (as in CI)
python -m ruff check app/ tests/
python -m pip_audit
```

## Pushing to GitHub

Before you push, make sure:

1. **Tests pass:** Run `python -m pytest tests/ -v --tb=short` (fix any failures).
2. **Security audit:** Run `python -m pip install pip-audit` then `python -m pip_audit` and address any critical vulnerabilities.
3. **No secrets in repo:** Ensure `.env` is in `.gitignore` and never commit API keys or passwords.
4. Then: `git add .` ‚Üí `git commit -m "Your message"` ‚Üí `git push origin main` (or your branch).

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

### What This Means

- ‚úÖ **Free to use** - Commercial and personal use allowed
- ‚úÖ **Modify freely** - Adapt to your needs
- ‚úÖ **Distribute** - Share your modifications
- ‚úÖ **Private use** - Use in proprietary projects
- ‚ö†Ô∏è **Attribution required** - Include the original license and copyright notice

The MIT License is one of the most permissive open-source licenses, making this template ideal for both learning and commercial use.

## Support

For issues and questions, please open an issue on the repository.

